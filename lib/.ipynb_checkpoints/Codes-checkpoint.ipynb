{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8ee22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_authors = df[\"author\"].value_counts().index.tolist()\n",
    "\n",
    "def make_histplot(stat, x_label, y_label, ax):\n",
    "    # Draw the histogram and fit a density plot.\n",
    "    sns.histplot(x=df.loc[df[\"author\"]==author].num_of_tokens, kde=True, ax=ax)\n",
    "    x=df.loc[df[\"author\"]==author].num_of_tokens\n",
    "    # get the y-coordinates of the points of the density curve.\n",
    "    dens_list = ax.get_lines()[0].get_data()[1]\n",
    "\n",
    "    # find the maximum y-coordinates of the density curve.\n",
    "    max_dens_index = dens_list.argmax()\n",
    "\n",
    "    # find the mode of the density plot.\n",
    "    mode_x = ax.get_lines()[0].get_data()[0][max_dens_index]\n",
    "    \n",
    "    # draw a vertical line at the mode of the histogram.\n",
    "    ax.axvline(mode_x, label='mode', color='blue', linestyle='dashed', linewidth=1.5)\n",
    "    \n",
    "    \n",
    "    # draw a vertical line at the mean of the histogram.\n",
    "    ax.axvline(x.mean(), label='mean',linestyle='dashed', color='red', linewidth=1.5)\n",
    "    \n",
    "\n",
    "    # draw a vertical line at the median of the histogram.\n",
    "    ax.axvline(x.median(), label='median',linestyle='dashed', color='green', linewidth=1.5)\n",
    "    ax.text(x.median(), 0.16, '{:.4f}'.format(x.median()))\n",
    "\n",
    "    ax.legend()\n",
    "    # Plot formatting\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(y_label)\n",
    "\n",
    "num_subplots = len(list_authors)\n",
    "ncols = 4\n",
    "\n",
    "nrows = 9\n",
    "fig, axes = plt.subplots(ncols=ncols, nrows=nrows, figsize=(ncols * 6, nrows * 5))\n",
    "#colors = plt.cm.tab10.colors\n",
    "for ax, author in zip(np.ravel(axes), list_authors):\n",
    "    make_histplot(author, author, 'number of appearance', ax)\n",
    "#for ax in np.ravel(axes)[num_subplots:]:  # remove possible empty subplots at the end\n",
    "    #ax.remove()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9171ec52",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df,x=\"num_of_tokens\",hue=\"school\",kind=\"kde\",fill=True, height=8.27, aspect=21.7/8.27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d24e417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sentence length split by school\n",
    "plt.figure(figsize=(16,5))\n",
    "sns.violinplot(x='school', y='num_of_tokens', data=df)\n",
    "plt.title('num_of_tokens - By School')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dc6d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_notion(tokenized_txt):\n",
    "    notions=[]\n",
    "    a=0\n",
    "    tokenized_txt=ast.literal_eval(tokenized_txt)\n",
    "    for i in tokenized_txt:\n",
    "        if i not in common_words:\n",
    "            notions.append(i)\n",
    "            a=a+1\n",
    "    return(a,notions)\n",
    "df[\"notions\"]=df[\"tokenized_txt\"].apply(create_notion)\n",
    "df[\"num_of_notions\"]=df[\"notions\"].apply(lambda x :x[0])\n",
    "df[\"notions_list\"]=df[\"notions\"].apply(lambda x :x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4982169a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_cmap = plt.get_cmap(\"viridis\")\n",
    "rescale = lambda y: (y - np.min(y)) / (np.max(y) - np.min(y))\n",
    "\n",
    "Uncommonness=(df.groupby(\"author\").sum()[\"num_of_notions\"]/df.groupby(\"author\").sum()[\"num_of_tokens\"]).sort_values()\n",
    "fig,ax=plt.subplots(figsize=(30,10))\n",
    "plt.bar(Uncommonness.index,Uncommonness.values,color=my_cmap(rescale(Uncommonness.values)))\n",
    "ax.tick_params(labelsize=15)\n",
    "for item in ax.xaxis.get_ticklabels():\n",
    "    item.set_rotation(40)\n",
    "plt.xlabel('Philosopher', fontsize=18)\n",
    "plt.ylabel('Uncommon word density', fontsize=16)\n",
    "plt.title(\"Difficultness of reading according to use of uncommon words\",fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891c9655",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "stopwords=set(common_words)\n",
    "schools = df[\"school\"].value_counts().index.tolist()\n",
    "for sc in schools:\n",
    "    df_temp = df[df.school==sc]\n",
    "    \n",
    "    print('School = ', sc.upper(), ':')\n",
    "    \n",
    "    # render wordcloud\n",
    "    text = \" \".join(txt for txt in df_temp.sentence_lowered)\n",
    "    wordcloud = WordCloud(stopwords=stopwords, max_font_size=50, max_words=500,\n",
    "                          width = 600, height = 400,\n",
    "                          background_color=\"white\").generate(text)\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "t2 = time.time()\n",
    "print('Elapsed time: ', np.round(t2-t1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07197a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "def SentimentAnlysis(sentence):\n",
    "    sentAnalyzer = SentimentIntensityAnalyzer() \n",
    "    sentDict = sentAnalyzer.polarity_scores(sentence)\n",
    "    \n",
    "    if sentDict['compound'] >= 0.05:\n",
    "        return \"positive\"\n",
    "    elif sentDict['compound'] <= -0.05 :\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "\n",
    "# Our Main Function\n",
    "def Analyzer(dataFrame, author):\n",
    "    df = dataFrame [dataFrame['author'] == author] \n",
    "        \n",
    "    #making a Corpus and finding sentiments\n",
    "    corpus = ''\n",
    "    numPostives = 0\n",
    "    numNegatives = 0\n",
    "    numNeutrals = 0\n",
    "    \n",
    "    for mem in df['sentence_lowered']:\n",
    "        corpus += mem\n",
    "    \n",
    "    for i in range (len(df)):\n",
    "        sent = (SentimentAnlysis(df['sentence_lowered'].iloc[i]))\n",
    "        if sent == \"positive\":\n",
    "            numPostives += 1\n",
    "        elif sent == \"negative\":\n",
    "            numNegatives += 1\n",
    "        else:\n",
    "            numNeutrals += 1\n",
    "    \n",
    "    plt.figure(figsize = (7, 7))\n",
    "    plt.pie([numPostives, numNegatives, numNeutrals], labels = ['positives', 'negatives', 'neutrals'], autopct='%1.2f%%')\n",
    "    plt.title('Sentiment Analysis for  Philosopher: ' + author)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
